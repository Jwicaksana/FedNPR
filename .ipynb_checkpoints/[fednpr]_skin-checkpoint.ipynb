{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf182b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, \\\n",
    "balanced_accuracy_score, confusion_matrix, roc_curve, auc, \\\n",
    "precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import copy\n",
    "import warnings \n",
    "import os \n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import umap\n",
    "from numpy import linalg as LA\n",
    "from helper import *\n",
    "from npr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c718fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPR_REG = True\n",
    "NUM_CLASS, CLUSTER, EMBED = 8, 4, 1280\n",
    "LAMBDA_ = 5e-2\n",
    "PERSONALIZED = True\n",
    "\n",
    "if PERSONALIZED:\n",
    "    head = ['classifier.1.weight','classifier.1.bias']\n",
    "else:\n",
    "    head = []\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "epochs = 80\n",
    "lr, momentum, wd = 1e-3, 0.9, 5e-4\n",
    "epochs_lr_decay  = [60, 70]\n",
    "lr_decay = 0.1\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "color_code = ['b','g','r','c','m','y','k']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ccab8",
   "metadata": {},
   "source": [
    "## select experiment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1773eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTH = '../datasets/isic/'\n",
    "PTH_gt = PTH +  'ISIC_2019_Training_GroundTruth.csv'\n",
    "PTH_metadata = PTH + 'ISIC_2019_Training_Metadata.csv'\n",
    "PTH_DATA = PTH + 'isic2019_processed/'\n",
    "PTH_split = '[isic]train_test_split'\n",
    "train_split = pd.read_csv(PTH_split)\n",
    "\n",
    "CLIENTS = ['BCN', 'vidir_molemax', 'vidir_modern', 'rosendahl', 'MSK4', 'vienna_dias']\n",
    "TOTAL_CLIENTS = len(CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee505f",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_per_center = dict()\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "\n",
    "\n",
    "global_train_img = []\n",
    "global_train_lbl = []\n",
    "global_test_img = []\n",
    "global_test_lbl = []\n",
    "for i in range(6):\n",
    "    split_per_center[i] = train_split[train_split['center'] == i]\n",
    "    lbl_ = split_per_center[i]['target']\n",
    "    \n",
    "    ## get the train and test split\n",
    "    train_idx = [True if f.split('_')[0]=='train' else \\\n",
    "                 False for f in split_per_center[i]['fold2']]\n",
    "    \n",
    "    test_idx = [True if f.split('_')[0]=='test' else \\\n",
    "                 False for f in split_per_center[i]['fold2']]\n",
    "\n",
    "    '''\n",
    "    testing\n",
    "    '''\n",
    "    train_data[CLIENTS[i]] = {\"image\":np.array([PTH_DATA+fname+'.jpg' for \\\n",
    "                                                    fname in split_per_center[i][train_idx]['image']]),\\\n",
    "                                  \"label\":np.array([int(lbl) for lbl in lbl_[train_idx]])}\n",
    "\n",
    "    test_data[CLIENTS[i]] = {\"image\":np.array([PTH_DATA+fname+'.jpg' for \\\n",
    "                                                    fname in split_per_center[i][test_idx]['image']]),\\\n",
    "                                  \"label\":np.array([int(lbl) for lbl in lbl_[test_idx]])}\n",
    "\n",
    "    global_train_img+= [PTH_DATA+fname+'.jpg' for fname in split_per_center[i][train_idx]['image']]\n",
    "    global_train_lbl+= [int(lbl) for lbl in lbl_[train_idx]]\n",
    "    \n",
    "    global_test_img+= [PTH_DATA+fname+'.jpg' for fname in split_per_center[i][test_idx]['image']]\n",
    "    global_test_lbl+= [int(lbl) for lbl in lbl_[test_idx]]\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    tmp_ = np.array([int(lbl) for lbl in lbl_])\n",
    "    print('center ', i, ':', np.unique(tmp_,return_counts=True))\n",
    "    clstype, clsamt = np.unique(tmp_, return_counts=True)\n",
    "    \n",
    "    \n",
    "    print('total data', len(tmp_))\n",
    "    print('train ratio: , ', np.sum(np.array(train_idx))/len(tmp_))\n",
    "    print()\n",
    "    \n",
    "train_data['global'] = {\"image\":np.array(global_train_img), \"label\":np.array(global_train_lbl)}\n",
    "test_data['global'] = {\"image\":np.array(global_test_img), \"label\":np.array(global_test_lbl)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9aea40",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Class for loading the #\n",
    "# skin dataset          #\n",
    "#########################\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "class cancer(Dataset):\n",
    "    def __init__(self, im_path, class_label, \\\n",
    "                 transformation=None, num_class=8):\n",
    "        self.data = im_path\n",
    "        self.label = class_label\n",
    "        self.transform = transformation\n",
    "        self.num_class = num_class\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = Image.open(self.data[idx])\n",
    "        if self.transform: # augmentation\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = Resize((224,224))(image)\n",
    "            image = TF.to_tensor(image)\n",
    "        label = self.label[idx]\n",
    "        label_onehot = torch.FloatTensor(self.num_class).zero_()\n",
    "        label_onehot[label] = 1.0\n",
    "        \n",
    "        return image, label, idx, label_onehot\n",
    "    \n",
    "input_transform_train = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    RandomRotation(degrees=(-10,10)),\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "input_transform_test = Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    Normalize(norm_mean, norm_std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b123ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "clsnm_, criterions_, loader = dict(), dict(), dict()\n",
    "_, clsnm_global = np.unique(train_data['global']['label'], return_counts=True)\n",
    "\n",
    "for cl_name in CLIENTS:\n",
    "    loader['train_'+cl_name] = DataLoader(cancer(train_data[cl_name]['image'], \\\n",
    "                                               train_data[cl_name]['label'], input_transform_train), \\\n",
    "                                        shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    loader['test_'+cl_name] = DataLoader(cancer(test_data[cl_name]['image'], \\\n",
    "                                               test_data[cl_name]['label'], input_transform_test), \\\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    cls, clsnm = np.unique(train_data[cl_name]['label'], return_counts=True)\n",
    "    clsnm_[cl_name] = np.zeros(NUM_CLASS)\n",
    "    for idx, i in enumerate(cls):\n",
    "        clsnm_[cl_name][i] = clsnm[idx]\n",
    "\n",
    "    criterions_[cl_name] = CustomLoss(clsnm_[cl_name], \\\n",
    "                                      CE_loss,device, balance=True)\n",
    "\n",
    "# binarize label\n",
    "label_binarizer = LabelBinarizer().fit(train_data['global']['label'].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc = 0.0\n",
    "epoch_best_avg = 0\n",
    "\n",
    "acc_train, loss_train, acc_test = dict(), dict(), dict() \n",
    "nets, optimizers, schedulers = dict(), dict(), dict()\n",
    "weights = dict()\n",
    "\n",
    "avg_auc_ = dict()\n",
    "prototypes_ = dict()\n",
    "\n",
    "index = [] \n",
    "#### additional storage ####\n",
    "test_acc_avg = []\n",
    "pc_avg_auc= []\n",
    "###############################################\n",
    "### federated model #####\n",
    "tmp = models.efficientnet_b0(pretrained=True)\n",
    "tmp.classifier[1] = nn.Linear(in_features=1280, \\\n",
    "                              out_features=NUM_CLASS, bias=True)\n",
    "\n",
    "nets['global'] = tmp\n",
    "nets['global'].to(device)\n",
    "##########################\n",
    "total = 0\n",
    "num_client = 0\n",
    "for key,_ in loader.items():\n",
    "    if key[:4] == 'test':\n",
    "        tmp = key[5:]\n",
    "        acc_train[tmp], loss_train[tmp] = [], []\n",
    "        acc_test[tmp] = []\n",
    "        avg_auc_[tmp] = []\n",
    "        #########################################\n",
    "        prototypes_[tmp] = npr(cluster=CLUSTER, classnum=NUM_CLASS,\\\n",
    "                         embed=EMBED).to(device)\n",
    "        weights[tmp] = len(loader['train_'+tmp].dataset)\n",
    "        total += weights[tmp]\n",
    "        num_client += 1\n",
    "        tmp_ = models.efficientnet_b0(pretrained=True)\n",
    "        tmp_.classifier[1] = nn.Linear(in_features=1280, out_features=NUM_CLASS, bias=True)\n",
    "        \n",
    "        nets[tmp] = tmp_\n",
    "        nets[tmp].to(device)\n",
    "        optimizers[tmp] = optim.Adam(nets[tmp].parameters(), lr=lr, \\\n",
    "           weight_decay=wd)\n",
    "        schedulers[tmp] = optim.lr_scheduler.MultiStepLR(optimizers[tmp],\n",
    "                                                      milestones=epochs_lr_decay,\n",
    "                                                      gamma=lr_decay)\n",
    "# assign federated average weight\n",
    "WEIGHTS_CL = np.zeros(num_client)\n",
    "for idx, cl_name in enumerate(CLIENTS):\n",
    "    WEIGHTS_CL[idx] = weights[cl_name]/total * 1.0\n",
    "\n",
    "for epoch in range(80):\n",
    "    index.append(epoch) \n",
    "    print(\"epoch number %d \" % epoch)\n",
    "    # aggregate model and download to client\n",
    "    aggr_fed(CLIENTS, WEIGHTS_CL, nets, fed_name='global', head=[])\n",
    "    copy_fed(CLIENTS, nets, fed_name='global', head=head)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    for client, _ in acc_train.items():\n",
    "        update_prototype(loader['train_'+client], nets[client], \\\n",
    "                        prototypes_[client], device)\n",
    "        criterion = criterions_[client]\n",
    "        schedulers[client].step() # step\n",
    "        train(loader['train_'+client], nets[client], optimizers[client], \\\n",
    "                  criterion, device,\\\n",
    "          acc_arr=acc_train[client], loss_arr=loss_train[client], \\\n",
    "          NPR_REG = True, LAMBDA_=0.05, prototypes_=prototypes_[client])\n",
    "        \n",
    "    avg_acc = 0.0\n",
    "    avg_auc = 0\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for client, _ in acc_test.items():\n",
    "        test(loader['test_'+client], nets[client], criterion, device, \\\n",
    "            acc_arr=acc_test[client], avg_auc=avg_auc_[client], \\\n",
    "            label_binarizer=label_binarizer, NUM_CLASS=NUM_CLASS)\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        avg_auc += avg_auc_[client][-1]\n",
    "        \n",
    "    test_acc_avg.append(avg_acc/TOTAL_CLIENTS)\n",
    "    pc_avg_auc.append(avg_auc/TOTAL_CLIENTS)\n",
    "    avg_acc = avg_acc/TOTAL_CLIENTS\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        epoch_best_avg = epoch\n",
    "                \n",
    "    clear_output(wait=True)\n",
    "    print('avg acc per client,',test_acc_avg[-1], best_avg_acc)\n",
    "    print('avg auc now:', pc_avg_auc[-1], \\\n",
    "         'avg auc best:', pc_avg_auc[epoch_best_avg])\n",
    "\n",
    "\n",
    "    plot_graphs(1, CLIENTS, index, acc_test, 'test acc')\n",
    "    plt.figure(2)\n",
    "    plt.plot(index, test_acc_avg, label= 'test acc per client')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d516900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e91aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
